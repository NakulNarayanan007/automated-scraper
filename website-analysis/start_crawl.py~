from boto.s3.key import Key
import datetime
import os,json,psycopg2,ast
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import os,sys,subprocess,time,json
import boto
import boto.s3
import sys,os
from boto.s3.key import Key
import smtplib
from email.MIMEMultipart import MIMEMultipart
from email.MIMEText import MIMEText

os.chdir('/home/sme-scraper-website')


DB_NAME=os.environ['DB_NAME']
DB_USER=os.environ['DB_USER']
DB_PASSWORD=os.environ['DB_PASSWORD']
DB_HOST=os.environ['DB_HOST']
params = {'database': DB_NAME,'user': DB_USER,'password': DB_PASSWORD,'host': DB_HOST,'port': '5432'}
db_con = None


# To create a virtual display in docker
os.system("Xvfb :1 -screen 0 1024x768x16 &> xvfb.log  &")
try:
#Create a database sessio
 db_con = psycopg2.connect(**params)
except psycopg2.DatabaseError as e:
 print ('Error %s' % e)

cursor = db_con.cursor()
try:
  cursor.execute("SELECT url_link,status FROM api_url ")
  rows = cursor.fetchall()
  start_urls=[]
  print"START"
  for row in rows:
    start_urls.append(row[0])

  for url in start_urls:
    cmd=['scrapy crawl sme -a link='+str(url)]
    with open('mylog.log', 'w') as logfile:
      pgm=[subprocess.Popen(c.split()) for c in cmd]
      time.sleep(25)

  try:
    cursor.execute("SELECT data FROM old_sme_scraper ")
    rows = cursor.fetchall()
    # code to measure the result
    if rows[0]:
      old_website_url=0
      old_image_count=0
      old_service_offered=0
      old_page_count=0
      old_time=0
      old_about_us=0
      old_latest_update=0
      old_responsive=0
      old_phone_numbers=0
      old_jobs_exist=0
      old_under_construction=0
      old_postal_addresses=0
      old_emails=0
      old_social_media_links=0
      old_blog_posts=0
      old_direct_phone_numbers=0
      old_direct_emails=0
      old_rss_feeds=0
      old_technologies=0
      old_blog_urls=0
      old_new_technologies=0

      new_website_url=0
      new_image_count=0
      new_service_offered=0
      new_page_count=0
      new_time=0
      new_about_us=0
      new_latest_update=0
      new_responsive=0
      new_phone_numbers=0
      new_jobs_exist=0
      new_under_construction=0
      new_postal_addresses=0
      new_emails=0
      new_social_media_links=0
      new_blog_posts=0
      new_direct_phone_numbers=0
      new_direct_emails=0
      new_rss_feeds=0
      new_technologies=0
      new_blog_urls=0
      new_new_technologies=0

      total_url=0
      cursor.execute("SELECT url_link FROM api_url ")
      urls = cursor.fetchall()
      print"PROCESSING......."
      for link in urls:
        url=link[0] 
        total_url=total_url+1
        cursor.execute("SELECT data FROM api_sme_scraper where url_id='"+url+"'")
        new_rows = cursor.fetchall()    
        for new_row in new_rows:
          new_data=json.loads(new_row[0])
        cursor.execute("SELECT data FROM old_sme_scraper where url_id='"+url+"'")
        old_rows = cursor.fetchall()    
        for old_row in old_rows:
          old_data=json.loads(old_row[0])
        try:
         if old_data["website_url"]:
           old_website_url=old_website_url+1
         if old_data["image_count"]:
            old_image_count=old_image_count+1
         if old_data["service_offered"]:
            old_service_offered=old_service_offered+1
         if old_data["page_count"]:
            old_page_count=old_page_count+1
         if old_data["time"]:
            old_time=old_time+1
         if old_data["about_us"]:
            old_about_us=old_about_us+1
         if old_data["latest_update"]:
            old_latest_update=old_latest_update+1
         if old_data["responsive"]:
            old_responsive=old_responsive+1
         if old_data["phone_numbers"]:
            old_phone_numbers=old_phone_numbers+1
         if old_data["jobs_exist"]:
            old_jobs_exist=old_jobs_exist+1
         if old_data["under_construction"]:
            old_under_construction=old_under_construction+1
         if old_data["postal_addresses"]:
            old_postal_addresses=old_postal_addresses+1
         if old_data["emails"]:
            old_emails=old_emails+1
         if old_data["social_media_links"]:
            old_social_media_links=old_social_media_links+1
         if old_data["blog_posts"]:
            old_blog_posts=old_blog_posts+1
         if old_data["direct_phone_numbers"]:
            old_direct_phone_numbers=old_direct_phone_numbers+1
         if old_data["direct_emails"]:
            old_direct_emails=old_direct_emails+1
         if old_data["rss_feeds"]:
            old_rss_feeds=old_rss_feeds+1
         if old_data["technologies"]:
            old_technologies=old_technologies+1
         if old_data["blog_urls"]:
            old_blog_urls=old_blog_urls+1
         if old_data["new_technologies"]:
            old_new_technologies=old_new_technologies+1

        except:
         pass

        try:
         if new_data["website_url"]:
            new_website_url=new_website_url+1
         if new_data["image_count"]:
            new_image_count=new_image_count+1
         if new_data["service_offered"]:
            new_service_offered=new_service_offered+1
         if new_data["page_count"]:
            new_page_count=new_page_count+1
         if new_data["time"]:
            new_time=new_time+1
         if new_data["about_us"]:
            new_about_us=new_about_us+1
         if new_data["latest_update"]:
            new_latest_update=new_latest_update+1
         if new_data["responsive"]:
            new_responsive=new_responsive+1
         if new_data["phone_numbers"]:
            new_phone_numbers=new_phone_numbers+1
         if new_data["jobs_exist"]:
            new_jobs_exist=new_jobs_exist+1
         if new_data["under_construction"]:
            new_under_construction=new_under_construction+1
         if new_data["postal_addresses"]:
            new_postal_addresses=new_postal_addresses+1
         if new_data["emails"]:
            new_emails=new_emails+1
         if new_data["social_media_links"]:
            new_social_media_links=new_social_media_links+1
         if new_data["blog_posts"]:
            new_blog_posts=new_blog_posts+1
         if new_data["direct_phone_numbers"]:
            new_direct_phone_numbers=new_direct_phone_numbers+1
         if new_data["direct_emails"]:
            new_direct_emails=new_direct_emails+1
         if new_data["rss_feeds"]:
            new_rss_feeds=new_rss_feeds+1
         if new_data["technologies"]:
            new_technologies=new_technologies+1
         if new_data["blog_urls"]:
            new_blog_urls=new_blog_urls+1
         if new_data["new_technologies"]:
            new_new_technologies=new_new_technologies+1

        except:
         pass
        

    n_groups = 21

    old_data = (old_website_url,old_image_count,old_service_offered,old_page_count,old_time,old_about_us,old_latest_update,old_responsive,old_phone_numbers,old_jobs_exist,old_under_construction,old_postal_addresses,old_emails,old_social_media_links,old_blog_posts,old_direct_phone_numbers,old_direct_emails,old_rss_feeds,old_technologies,old_blog_urls,old_new_technologies)
    #std_old_data = (2, 3, 4, 1, 2)

    new_data = (new_website_url,new_image_count,new_service_offered,new_page_count,new_time,new_about_us,new_latest_update,new_responsive,new_phone_numbers,new_jobs_exist,new_under_construction,new_postal_addresses,new_emails,new_social_media_links,new_blog_posts,new_direct_phone_numbers,new_direct_emails,new_rss_feeds,new_technologies,new_blog_urls,new_new_technologies)
    #std_new_data = (3, 5, 2, 3, 3)

    fig, ax = plt.subplots(figsize=(15,15))

    index = np.arange(n_groups)
    bar_width = 0.2

    opacity = 0.4
    error_config = {'ecolor': '0.3'}
     
    
    rects1 = plt.bar(index, old_data, bar_width,alpha=opacity,color='b',error_kw=error_config,label='Old Scraper Data')

    rects2 = plt.bar(index + bar_width, new_data, bar_width,alpha=opacity,color='r',error_kw=error_config,label='New Scraper Data')



    plt.xlabel('Data Points')
    plt.ylabel('Scores')
    title='Website Analysis Of '+str(total_url)+'Sites'
    plt.title(title)
    plt.xticks(index + bar_width, ('website_url', 'image_count', 'service_offered', 'page_count','time','about_us','latest_update','responsive','phone_numbers','jobs_exist','under_construction','postal_addresses','emails','social_media_links','blog_posts','direct_phone_numbers','direct_emails','rss_feeds','technologies','blog_urls','new_technologies'), fontsize = 10,rotation=90)
    plt.legend(loc='best')
    plt.tight_layout()
    plt.savefig('/measure.png')
    print"SAVED..."
    now = datetime.datetime.now()
    time_stamp="measure_data_"+str(now.year)+"_"+str(now.month)+"_"+str(now.day)+"_"+str(now.hour)+"_"+str(now.minute)
    AWS_ACCESS_KEY_ID=os.environ['AWS_ACCESS_KEY_ID']
    AWS_SECRET_ACCESS_KEY=os.environ['AWS_SECRET_ACCESS_KEY']
    #AWS_ACCESS_KEY_ID = 'AKIAIU2A7CGHOOMN43IQ'
    #AWS_SECRET_ACCESS_KEY = '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'

    print AWS_SECRET_ACCESS_KEY
    REGION_HOST = 's3.eu-central-1.amazonaws.com'
    bucket_name = 'sme-website-scraper-prod'
    conn = boto.connect_s3(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,host=REGION_HOST)

    bucket = conn.get_bucket(bucket_name)

    testfile = "/measure.png"
    print 'Uploading %s to Amazon S3 bucket %s'

    k = Key(bucket)
    k.key = time_stamp
    k.set_contents_from_filename(testfile)

  except:
    pass
except:
  pass
if db_con:
  db_con.close()


   
